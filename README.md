# README

[30 Days of ML](https://www.kaggle.com/thirty-days-of-ml) was an event ran by Kaggle with the purpose of getting novice data scientists into Machine Learning and Kaggle competitions. According to the competition overview, "the dataset used for this competition is synthetic (and generated using a CTGAN), but based on a real dataset. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features."

## Files

* data -  original competition data files
  * sample_submission.csv - example competition submission file
  * test.csv - the test set on which our model will be evaluated
  * train.csv - the training data for our model
* output - output generated by optuna hyperparameter search
* submissions - predictions
* lightgbm_search.py - python script for hyperparameter search for LightGBM
* optuna_analysis.py - jupyter notebook evaluating the optuna output

## About

The majority of the work I did for this 15 day competition was done using mostly Kaggle notebooks originally. I tested out mostly GPU-enabled XGBoost models, however my original preprocessing and cross-validation schemes were leaking validation data into my models and/or training data.

By this point, I had used up most of my weekly GPU accelerator quota and XGBoost ran far too slowly with CPU so I switched over to LightGBM. It turned out to give me better results and could run on my local machine.
